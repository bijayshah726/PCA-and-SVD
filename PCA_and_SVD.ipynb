{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e397272c",
   "metadata": {},
   "source": [
    "# I. Creating PCA function from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a5441",
   "metadata": {},
   "source": [
    "Before you begin this tutorial, I would recommend you spend some time on this paper.\n",
    "https://arxiv.org/pdf/1404.1100\n",
    "\n",
    "It is a great paper by Jon Shlens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6133e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#np.set_printoptions(precision=8) #to set precision if needed\n",
    "\n",
    "\n",
    "#Function for finding PCA \n",
    "def pca_func(X):\n",
    "    '''\n",
    "    This function takes an array of n*m matrix where m is the type of data and n is the number of data points\n",
    "    and output the eigen vectors, eigen values and transformed data using the PCA method\n",
    "    '''\n",
    "    #calling standardize function to make data normalized with 0 mean\n",
    "    X=standardize(X)\n",
    "\n",
    "    n, m = X.shape\n",
    "    # Compute covariance matrix\n",
    "    C = np.dot(X.T, X) / (n-1)\n",
    "    # Eigen decomposition\n",
    "    eigen_vals, eigen_vecs = np.linalg.eig(C)\n",
    "    # Project X onto PC space\n",
    "    X_pca = np.dot(X, eigen_vecs)\n",
    "    return eigen_vecs,eigen_vals,X_pca\n",
    "\n",
    "\n",
    "#Function for standardization\n",
    "def standardize(X):                       # X is the input data\n",
    "    mu=sum(X)/len(X)                      # Mean across columns\n",
    "    var=sum((X-mu)**2)/len(X)             #Variance of the data\n",
    "    z=(X-mu)/(var**0.5)                   #Normalization\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580fb1f",
   "metadata": {},
   "source": [
    "# Data for understanding the concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136997d9",
   "metadata": {},
   "source": [
    "First data will contain the length, breadth and height of the cube. We all know that they are equal and hence only one column is sufficient to represent the data. PCA and SVD just extract this feature.\n",
    "\n",
    "Second data will be about speed(m/s) , distance covered(m) and time(in seconds).We will assume that we don't know the relationship between speed, time and distance (but actuaaly we do). In this, we know that two columns should be sufficient to represent the entire data. PCA and SVD will just show this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d78939f",
   "metadata": {},
   "source": [
    "# 1. Understanding Cube dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd14db4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal vectors\n",
      " [[ 5.77350269e-01  5.77350269e-01  5.77350269e-01]\n",
      " [-4.43374929e-17 -7.07106781e-01  7.07106781e-01]\n",
      " [ 8.16184513e-01 -4.27640407e-01 -3.88544106e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Input Data for Cube\n",
    "\n",
    "\n",
    "X=np.array([\n",
    "    [2,2,2],\n",
    "    [4,4,4],\n",
    "    [1,1,1],\n",
    "    [0.2,0.2,0.2],\n",
    "    [5,5,5],\n",
    "    [100,100,100],\n",
    "    [9,9,9]\n",
    "]\n",
    ")\n",
    "\n",
    "E,eigen_values,Y=pca_func(X)\n",
    "P=E.T\n",
    "print(\"Principal vectors\\n\", P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878304f3",
   "metadata": {},
   "source": [
    "## Transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1f0e2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data Y\n",
      " [[-7.83227402e-01  2.58333199e-17  6.97948461e-17]\n",
      " [-6.80940241e-01  5.70806499e-17  5.35883500e-17]\n",
      " [-8.34370982e-01  2.98358107e-17  6.71138288e-17]\n",
      " [-8.75285846e-01  4.73918015e-17  1.02589412e-16]\n",
      " [-6.29796661e-01  5.30781591e-17  5.62693673e-17]\n",
      " [ 4.22884347e+00 -2.50287937e-16 -2.49815787e-16]\n",
      " [-4.25222339e-01  2.77555756e-17  4.16333634e-17]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Transformed data Y\\n\",Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cfc8c3",
   "metadata": {},
   "source": [
    "## Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b6aa2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding variance \n",
    "def variance(data):\n",
    "    no_of_data=len(data)\n",
    "    mu=np.sum(data,axis=0)/no_of_data               #mean across columns\n",
    "    var=sum((data-mu)**2)/no_of_data\n",
    "    var_explained=var/sum(var)\n",
    "    return var,var_explained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d3db94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance along principle direction for P. First array is variance and second is variance explained.\n",
      "\n",
      "(array([8.21730110e-33, 3.33333333e-01, 3.33333333e-01]), array([1.23259516e-32, 5.00000000e-01, 5.00000000e-01]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Variance along principle direction for P. First array is variance and second is variance explained.\\n\")\n",
    "print(variance(E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb6f0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance in the transformed data \n",
      "\n",
      "(array([3.00000000e+00, 1.04687207e-32, 1.24617513e-32]), array([1.00000000e+00, 3.48957355e-33, 4.15391711e-33]))\n"
     ]
    }
   ],
   "source": [
    "print(\"Variance in the transformed data \\n\")\n",
    "print(variance(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ff45c",
   "metadata": {},
   "source": [
    "## II. PCA using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d547910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained\n",
      " [1.00000000e+00 5.10610875e-32 1.09958905e-63]\n",
      "\n",
      "P matrix using sklearn PCA\n",
      " [[ 0.57735027  0.57735027  0.57735027]\n",
      " [-0.81649658  0.40824829  0.40824829]\n",
      " [-0.          0.70710678 -0.70710678]]\n",
      "\n",
      "Transformed data usign sklearn PCA\n",
      "\n",
      "[[-7.83227402e-01 -8.17702297e-17  2.43705824e-17]\n",
      " [-6.80940241e-01 -5.82322590e-17  4.20779239e-17]\n",
      " [-8.34370982e-01 -9.60858366e-17  7.91574827e-17]\n",
      " [-8.75285846e-01 -8.43152130e-17  6.42240838e-17]\n",
      " [-6.29796661e-01 -4.39166521e-17  4.28021748e-17]\n",
      " [ 4.22884347e+00  2.95463264e-16 -2.42820275e-16]\n",
      " [-4.25222339e-01 -4.16333634e-17  2.77555756e-17]]\n",
      "\n",
      "Comparison of this transformed data with that of direct PCA function\n",
      "\n",
      "[[-1.11022302e-16 -1.07603550e-16 -4.54242637e-17]\n",
      " [-1.11022302e-16 -1.15312909e-16 -1.15104261e-17]\n",
      " [-1.11022302e-16 -1.25921647e-16  1.20436539e-17]\n",
      " [-1.11022302e-16 -1.31707015e-16 -3.83653281e-17]\n",
      " [ 0.00000000e+00 -9.69948112e-17 -1.34671926e-17]\n",
      " [ 0.00000000e+00  5.45751201e-16  6.99551233e-18]\n",
      " [-5.55111512e-17 -6.93889390e-17 -1.38777878e-17]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_standard=standardize(X)                 #standardizing data before use\n",
    "pca = PCA(n_components=3)\n",
    "x=pca.fit(X_standard)\n",
    "print(\"Variance explained\\n\",pca.explained_variance_ratio_)\n",
    "print(\"\\nP matrix using sklearn PCA\\n\",pca.components_)\n",
    "\n",
    "#printing transformed data from sklearn\n",
    "print(\"\\nTransformed data usign sklearn PCA\\n\")\n",
    "print(pca.transform(X_standard))\n",
    "print(\"\\nComparison of this transformed data with that of direct PCA function\\n\")\n",
    "print(pca.transform(X_standard)-Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd96dc",
   "metadata": {},
   "source": [
    "### Explanation for PCA\n",
    "This function is almost similar to the the function described above. Matrix P is the transpose of Eigen vector.\n",
    "\n",
    "We can see that alomst 100% of the variance is explained by just the first column (first Principal Component).\n",
    "Also, comparison of data can be tricky in other cases as the inbuilt function arranged the PCs in the order of decreasing importance (variance explained). In this case, as it is only one that is main; that need not be checked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d77f9a",
   "metadata": {},
   "source": [
    "## III.SVD using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb009fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data using numpy SVD \n",
      "X_svd= U * Sigma\n",
      " \n",
      "[[-7.83227402e-01 -1.01898173e-15  1.01092274e-34]\n",
      " [-6.80940241e-01  3.05298860e-17  1.49771214e-31]\n",
      " [-8.34370982e-01 -1.36562109e-17  6.46928446e-33]\n",
      " [-8.75285846e-01  2.30241171e-17 -2.64395044e-33]\n",
      " [-6.29796661e-01  1.35623235e-17 -4.97835079e-33]\n",
      " [ 4.22884347e+00 -1.79148590e-16  2.35617576e-32]\n",
      " [-4.25222339e-01  5.67749558e-18 -5.58216822e-33]]\n",
      "\n",
      "Comparison of this transformed data with that of direct PCA function\n",
      "\n",
      "[[ 1.88737914e-15 -1.04481505e-15 -6.97948461e-17]\n",
      " [ 2.22044605e-16 -2.65507638e-17 -5.35883500e-17]\n",
      " [ 2.22044605e-16 -4.34920216e-17 -6.71138288e-17]\n",
      " [ 3.33066907e-16 -2.43676845e-17 -1.02589412e-16]\n",
      " [ 2.22044605e-16 -3.95158356e-17 -5.62693673e-17]\n",
      " [-1.77635684e-15  7.11393469e-17  2.49815787e-16]\n",
      " [ 1.11022302e-16 -2.20780800e-17 -4.16333634e-17]]\n",
      "\n",
      "Variance Explained\n",
      "(array([3.00000000e+00, 1.26707663e-31, 2.73165457e-63]), array([1.00000000e+00, 4.22358875e-32, 9.10551522e-64]))\n"
     ]
    }
   ],
   "source": [
    "n,m=X.shape\n",
    "\n",
    "from numpy.linalg import svd \n",
    "U, S, Vt = svd(X_standard, full_matrices=True)\n",
    "Sigma = np.zeros((n, m), dtype=float)\n",
    "Sigma[:m, :m] = np.diag(S)\n",
    "print(\"Transformed data using numpy SVD \\nX_svd= U * Sigma\\n \")\n",
    "X_svd=np.dot(U, Sigma)\n",
    "print(X_svd)\n",
    "\n",
    "print(\"\\nComparison of this transformed data with that of direct PCA function\\n\")\n",
    "print(X_svd-Y)\n",
    "\n",
    "print(\"\\nVariance Explained\")\n",
    "print(variance(X_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04dee0d",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "This also gives the same result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635178c4",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a275ee4",
   "metadata": {},
   "source": [
    "In all the three method we see that variance explained is almost 100% for only 1 PC. Only one column is sufficient to represent the side of the cube."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8e392",
   "metadata": {},
   "source": [
    "# 2. Understanding random data of speed, distance and time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "799bc207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Principal vectors\n",
      " [[-0.70327211  0.10611967  0.70295587]\n",
      " [ 0.70726305  0.00426342  0.70693762]\n",
      " [-0.07202299 -0.99434423  0.07805287]]\n",
      "\n",
      "Transformed data Y\n",
      " [[-1.81847814e-02 -7.11561189e-01  7.28675220e-01]\n",
      " [ 2.28274129e-02 -4.71300812e-01 -5.07166800e-01]\n",
      " [-2.42918290e-02 -7.01895851e-01  8.83221039e-01]\n",
      " [-1.51773770e-02 -6.85179835e-01  1.02390596e+00]\n",
      " [ 3.21008836e-03  3.42690255e+00  2.83234488e-01]\n",
      " [-4.18741313e-02 -1.93712932e-01 -2.06444317e+00]\n",
      " [ 7.34906174e-02 -6.63251929e-01 -3.47426738e-01]]\n"
     ]
    }
   ],
   "source": [
    "#Input Data for Cube\n",
    "#distance is in first column, time in 2nd column and speed in 3rd.\n",
    "\n",
    "X=np.array([\n",
    "    [1,2,0.5],\n",
    "    [40,10,4],\n",
    "    [1,1,1],\n",
    "    [0.2,0.1,2],\n",
    "    [500,5,100],\n",
    "    [100,20,5],\n",
    "    [9,9,1]\n",
    "]\n",
    ")\n",
    "\n",
    "E,eigen_values,Y=pca_func(X)\n",
    "P=E.T\n",
    "print(\"Principal vectors\\n\", P)\n",
    "\n",
    "print(\"\\nTransformed data Y\\n\",Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2de515b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance in the transformed data \n",
      "\n",
      "(array([1.26240588e-03, 1.98738027e+00, 1.01135733e+00]), array([4.20801960e-04, 6.62460089e-01, 3.37119109e-01]))\n"
     ]
    }
   ],
   "source": [
    "# Finding variance \n",
    "def variance(data):\n",
    "    no_of_data=len(data)\n",
    "    mu=np.sum(data,axis=0)/no_of_data               #mean across columns\n",
    "    var=sum((data-mu)**2)/no_of_data\n",
    "    var_explained=var/sum(var)\n",
    "    return var,var_explained\n",
    "print(\"Variance in the transformed data \\n\")\n",
    "print(variance(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca726d5",
   "metadata": {},
   "source": [
    "## II. PCA using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb83949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance explained\n",
      " [6.62460089e-01 3.37119109e-01 4.20801960e-04]\n",
      "\n",
      "P matrix using sklearn PCA\n",
      " [[ 0.70726305  0.00426342  0.70693762]\n",
      " [ 0.07202299  0.99434423 -0.07805287]\n",
      " [-0.70327211  0.10611967  0.70295587]]\n",
      "\n",
      "Transformed data usign sklearn PCA\n",
      "\n",
      "[[-7.11561189e-01 -7.28675220e-01 -1.81847814e-02]\n",
      " [-4.71300812e-01  5.07166800e-01  2.28274129e-02]\n",
      " [-7.01895851e-01 -8.83221039e-01 -2.42918290e-02]\n",
      " [-6.85179835e-01 -1.02390596e+00 -1.51773770e-02]\n",
      " [ 3.42690255e+00 -2.83234488e-01  3.21008836e-03]\n",
      " [-1.93712932e-01  2.06444317e+00 -4.18741313e-02]\n",
      " [-6.63251929e-01  3.47426738e-01  7.34906174e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "X_standard=standardize(X)                 #standardizing data before use\n",
    "pca = PCA(n_components=3)\n",
    "x=pca.fit(X_standard)\n",
    "print(\"Variance explained\\n\",pca.explained_variance_ratio_)\n",
    "print(\"\\nP matrix using sklearn PCA\\n\",pca.components_)\n",
    "\n",
    "#printing transformed data from sklearn\n",
    "print(\"\\nTransformed data usign sklearn PCA\\n\")\n",
    "print(pca.transform(X_standard))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94f7de7",
   "metadata": {},
   "source": [
    "## SVD using numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1641af19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data using numpy SVD \n",
      "X_svd= U * Sigma\n",
      " \n",
      "[[-7.11561189e-01 -7.28675220e-01 -1.81847814e-02]\n",
      " [-4.71300812e-01  5.07166800e-01  2.28274129e-02]\n",
      " [-7.01895851e-01 -8.83221039e-01 -2.42918290e-02]\n",
      " [-6.85179835e-01 -1.02390596e+00 -1.51773770e-02]\n",
      " [ 3.42690255e+00 -2.83234488e-01  3.21008836e-03]\n",
      " [-1.93712932e-01  2.06444317e+00 -4.18741313e-02]\n",
      " [-6.63251929e-01  3.47426738e-01  7.34906174e-02]]\n",
      "\n",
      "Variance Explained\n",
      "(array([1.98738027e+00, 1.01135733e+00, 1.26240588e-03]), array([6.62460089e-01, 3.37119109e-01, 4.20801960e-04]))\n"
     ]
    }
   ],
   "source": [
    "n,m=X.shape\n",
    "\n",
    "from numpy.linalg import svd \n",
    "U, S, Vt = svd(X_standard, full_matrices=True)\n",
    "Sigma = np.zeros((n, m), dtype=float)\n",
    "Sigma[:m, :m] = np.diag(S)\n",
    "print(\"Transformed data using numpy SVD \\nX_svd= U * Sigma\\n \")\n",
    "X_svd=np.dot(U, Sigma)\n",
    "print(X_svd)\n",
    "\n",
    "print(\"\\nVariance Explained\")\n",
    "print(variance(X_svd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12606a92",
   "metadata": {},
   "source": [
    "## Explanation \n",
    "Here, we can see that PC1 can explain 66.25% of data while PC2 can explain 33.72% of data. Two of them are close to 100%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0fd14e4",
   "metadata": {},
   "source": [
    "# Note\n",
    "It is important to note that all these data are random and it actually doesn't make sense to use PCA and SVD to modify the data as we already have the raw data and it can explain the info much more accurately and it is easy to explain."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
